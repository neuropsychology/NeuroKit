---
title: '**NeuroKit2: A Python Toolbox for Neurophysiological Signal Processing**'
shorttitle        : "NeuroKit2"
author:
  - name          : "Dominique Makowski"
    affiliation   : " 1,*"
    corresponding : no    # Define only one corresponding author
    address       : "HSS 04-18, 48 Nanyang Avenue, Singapore"
    email         : "dmakowski@ntu.edu.sg"
  - name          : "Tam Pham"
    affiliation   : " 1"
  - name          : "Zen J. Lau"
    affiliation   : " 1"
  - name          : "Jan C. Brammer"
    affiliation   : " 2"
  - name          : "Hung Pham"
    affiliation   : " 3"
  - name          : "Francois Lespinasse"
    affiliation   : " 4"
  - name          : "Christopher Sch\\\"{o}lzel"
    affiliation   : " 5"
  - name          : "S.H. Annabel Chen"
    affiliation   : " 1, 6, 7"
affiliation:
  - id            : "1"
    institution   : "School of Social Sciences, Nanyang Technological University, Singapore"
  - id            : "2"
    institution   : "???"
  - id            : "3"
    institution   : "???"
  - id            : "4"
    institution   : "Departement de psychologie, Universite de Montreal, Montreal, Canada"
  - id            : "5"
    institution   : "Life Science Informatics, THM University of Applied Sciences, Gisslen, Germany"
  - id            : "6"
    institution   : "Centre for Research and Development in Learning, Nanyang Technological University, Singapore"
  - id            : "7"
    institution   : "Lee Kong Chian School of Medicine, Nanyang Technological University, Singapore"
authornote: |
  * Correspondence concerning this article should be addressed to Dominique Makowski (HSS 04-18, 48 Nanyang Avenue, Singapore; dmakowski@ntu.edu.sg).
abstract: |
   NeuroKit2 is an open-source user-friendly Python package dedicated to neurophysiological signal processing. It developed from a collaborative project aimed at offering programming ease both for novice and advanced users to perform elaborate analyses of electrocardiogram (ECG), respiratory (RSP), electrodermal activity (EDA), and electromyography (EMG) data. It comprises of a consistent set of user-friendly, high-level functions that implement an all-in-one cleaning, preprocessing, and processing pipeline with sensible defaults. At the same time, greater flexibility and parametric control can be achieved by using Neurokit2's mid-level functions to build a custom analysis pipeline. (talk about novelty? or cutting-edge measure extraction? or reproducibility?)
keywords          : "Neurophysiology, Biosignals, Python, ECG, EDA, EMG, RSP"
wordcount         : ""
bibliography      : ["bibliography.bib"]
floatsintext      : yes
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output:
  papaja::apa6_pdf:
    keep_tex: FALSE
    latex_engine: xelatex
  papaja::apa6_word:
    keep_tex: FALSE
header-includes:
   - \usepackage[labelfont=bf, font={color=gray,small}]{caption}
   - \usepackage{float}
   - \usepackage[document]{ragged2e}
editor_options:
  chunk_output_type: console
---

\justify

```{r r_setup, include = FALSE, warning=FALSE, message=FALSE}
library("papaja")
library("kableExtra")
options(knitr.kable.NA = 'None')

library(tidyverse)
library(easystats)

# Setup python - you need to change the path to your python distribution
library(reticulate)
reticulate::use_python("D:/Downloads/WPy64-3810/python-3.8.1.amd64/")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
```




<!-- Research gap -->
As studies show that cognitive processes are reliant on whole living body activity, multiple areas of research studying human subjects<!--the fields of cognitive neuroscience and psychology--> look onto neurophysiological methods to investigate how brain-body interactions relates to behavior [@kiverstein2015embodied]<!--felt like a ref was needed to support the claim that it is de facto increasing-->. Effectively, these methods can litteraly reveal vital information about subjects as they can describe isolated responses of the oragnism as well as its organized visceral rhythms (circa- and infra-dians). An examplary case would that of HRV, being proposed to serve as biomarker for top-down self-regulation [@holzman2017heart]. Other reasons for the popularity of these methods can include low monetary cost (compared with imaging techniques, such as fMRI and MEG), high user convenience (e.g., portability) and wide availability (e.g., in "smart health devices"). <!-- easier to attain ecological validity ?-->. Along these developments in neurophysiology, signal processing and computational data science opened-up possibilities of analyses. For instance, open-sourced implementations of processing pipelines benchmarking and transparent computations of measurements (especially important for the case of nonlinear measurements)<!--pushing like never before the horizon of possibilities and opportunities -- what opportunities are talking about? providing examples would be great (e.g. valid and transparent implementations of complexity measures, or myriad of preprocessing and analysis pipelines have been developed)-->. However, as these methods are often not easily accessible and user-friendly, neurophysiological data processing remains a challenge for many researchers without a formal experience in programming.

<!-- Response to gap -->
*NeuroKit2* aims at addressing this challenge by offering a free and user-friendly solution for neurophysiological data processing. It is an open-source Python package, developed in a collaborative environment that continues to welcome contributors from different countries and fields. Historically, *NeuroKit2* is the re-forged successor *NeuroKit.py* (*https://github.com/neuropsychology/NeuroKit.py*), a PhD side project that ended up attracting a lot of users and success (248 GitHub stars as of 09-04-2020). The new version takes on its best features and design choices, and re-implements them in a professional and well-thaught way. It aims at being 1) accessible and well-documented, 2) reliable and cutting-edge, and most importantly, 3) robust and flexible.

<!-- Accessibility and documentation -->
The package is available for Python 3 [@python3] and thus users benefit from existing tutorials and large online community. It is also relatively lightweight, using mainly standard dependencies [@scipy] such as *NumPy*, *pandas*, *SciPy*, *scikit-learn* and *MatplotLib* (with an additional system of optional dependencies), enabling its use as a dependency in other software. The package source code is available under a permissive license on GitHub (*https://github.com/neuropsychology/NeuroKit*); along with its documentation, automatically built and hosted at *https://neurokit2.readthedocs.io/*. Documentation follows a standardized structure [@sphinx] in order to be rendered from the code. Apart from a guide for installation as well as contribution, and a decription of the package's functions, the documentation also includes several cloud-based interactive examples. These are built in a tutorial-like manner, providing a walk-through on how to address specific issues (for instance, how to extract and visualize individual heartbeats). New examples can be easily added by users simply by uploading a Python notebook file to the repository. This notebook will then be automatically rendered in a static form in the documentation webpage, and also accessible to everyone who wishes to interact with its content (no prior installation needed) via a *Binder* environment [@binder2018]. <!--(maybe referencing our binder would be a good idea at this point) This notebook file will be automatically transformed into a webpage and displayed on the website--> This ensures a transparent and evolutive documentation. The accessibility for newcomers is also reinforced by the issue tracker of GitHub, where users can create public issues to inquire for help.

<!-- Reliability and Evolution -->
The package aims at being reliable and trustworthy, that is implementations of peer-reviewed processing techniques and pipelines. It also means that the contributors are dedicated to ensuring its stability using continuous integration tools like Travis [@travis]. NeuroKit's functions are also tested against existing implementations of established reference software such as *BioSPPy* [@biosppy], *hrv* [*under review*](https://github.com/openjournals/joss-reviews/issues/1867), *PySiology* [@PySiology], *HeartPy* [@HeartPy], *systole* [@Systole] or *nolds* [@nolds]. The code itself includes a comprehensive test suite to ensure stability and prevent error. Moreover, the issue tracker allows users to easily report any bugs and track their fixation. Thanks to its collaborative and open developpment, as well as its modular organization, *NeuroKit2* is being developped with a longterm perspective in mind, aiming at remaining cutting-edge through its ability to evolve, adapt, and integrate new methods as they are emerging.

<!-- - Powerful and flexible: API -->
Finally, we believe that the design philosophy contributes to a robust  <!--powerful or robust | parentheses at end of sections are not super great; pushing them in the next ?--> yet flexible user interface (API), which is described below. We will then present the most common use-cases of NeuroKit through examples using Python scripts, namely with data from event-related and resting state paradigms. We will conclude by discussing how those examples reveal the way Neurokit addresses reproducibility issues in neurophysiological methods implementations. We will open up on a more foundational and theoretical note by appreciating how accessibility and continuous integration could benefit to new-coming applications of these methods. <!--foundational and theoretic issues : to answer to first statement which is about the growing use of this methods to describe human behavioral, cognitive and affective processes [embodiement, intricacies affectivity and cognition]-->

# Design Philosophy

We cannot stress this enough : *NeuroKit2* aims at being accessible to beginners and, at the same time, offering a maximal level of control to experienced users. Robustness and flexibility of the API are achieved by allowing beginning users to implement complex processing and analyses pipelines with very few functions, while still enabling fine-tuned control and precision to more experienced users. This trade-off is allowed by the implementation of 3 abstract levels of functions.


## Low-level: Signal Processing Base Utilities

The basic building blocks are functions to facilitate general signal processing, i.e., to do filtering, resampling, interpolating, peak detection, etc. These functions are signal-agnostic, and include a lot of tweakable parameters. For instance, one can change the filtering method, frequencies, order etc. Most of these functions are based on validated algorithms present in *scipy* [@scipy]. Examples of such functions include `signal_filter()`, `signal_interpolate()`, `signal_resample()`, `signal_detrend()`. `signal_findpeaks()`.

## Mid-level: Neurophysiological Processing Steps

The signal processing utilities are then used by functions specific to different types of physiological signals. These functions aim at taking care of specific steps of physiological data processing, such as cleaning, peak detection, phase classification or rate computation. Critically, for each type of signals (ECG, RSP, EDA, EMG, PPG, and M/EEG.), the same function names are called (in the form `signaltype_functiongoal()`) to achieve equivalent goals, such as `*_clean()`, `*_findpeaks()`, `*_process()`, `*_plot()` (replace the star with the signal type, e.g., `ecg_clean()`), making it intuitive and consistent to accross different modalities.

For example, the `rsp_clean()` function uses `signal_filter()` and `signal_detrend()`, with different possible sets of default parameters that can be switched via a "method" argument (corresponding to different published or validated pipelines). For instance, setting `method="khodadad2018"` will use the cleaning workflow described in @khodadad2018optimized. If a user wants to build its own custom cleaning function, he or she can use a `*_clean()` function as a template to choose the appropriate set of parameters in low-level signal processing tools (this process is also examplified in the documentation).


## High-level Wrappers for Processing and Analysis

Finally, these steps are assembled in front-end "master" functions. For instance, the `ecg_process()` function uses `ecg_clean()`, `ecg_findpeaks()`, `ecg_rate()`. A specific processing pipeline can be selected via the `method` argument of the given function. This `method` is propagated throughout its different subsets in lower-level functions. These modality-specific processing functions allow begginners to compare the outcomes of different pipelines on their data. It facilitates important and time-consuming steps in reproducible research such as validation of data preparation and quality control [@quintana2016guidelines].

<!--Last but not least ; not sure this expression is quite the tone we're looking for--> The package includes meta-functions (e.g., `bio_process`) that allows the combined processing of multiple types of signals at once. As a result, the existence of these high-level functions allows for reproducible and easy to implement biosignal processing pipelines. Performing all of the steps of physiological preprocessing and processing with sensible defaults, researchers who are not confortable in programming can run their pipeline writing one line of code (e.g., `bio_process(ecg=ecg_signal, eda=eda_signal, method='neurokit')`). We find this can be rewarding for begginners that, without prior expertise in programming, one can perform cutting-edge feature extractions and visualizations. In fact, for these begginners, it may demistify the usage of "pure programming tools" for their analyses (as opposed to Statistics and biosignal software using graphical interface such as SPSS, Kubios and Acqknowledge). Perhaps, it would also support the integrity of empirical research as it provides all-in-one, in contrast to software-specific pipelines. Importantly however, more advanced users can very easily build their own custom analysis pipeline by using the mid-level functions. Thus, the code structure offers more control over the parameters of pipelines at the cost of learning Python basics. We believe this calibrated trade-off between flexibility and robustness can result into motivating begginners to gradually move away from costly proprietary softwares that refrain reproducible research. And at the same time, one may think that the collaborative environment provided by NeuroKit will also drive them to learn how to build their own custom pipeline.


# Example


We will present two examples that illustrate the most common use-cases of *Neurokit*. Their implementation are broken down along with their datasets. Firstly, we will show how simple it is to compute contrast between experimental conditions using data from event-related paradigm. In this example, one would be interested in the momentarily short-term physiological response to specific stimuli. The second example provided shows how *Neurokit* can extract longer-term characteristics (features) of  physiological activity, namely Heart Rate Variability (HRV), phasic/tonic components Electrodermal Activity (EDA). Datasets are provided along with the package, and examples are available in their interactive and static versions.


## Event-related Paradigm

The dataset contains ECG, RSP and EDA signals recorded with one participant to whom was presented four emotional images (from the NAPS database; @marchewka2014nencki), which is a typical experimentation in psychology. The dataset is shortened as well as downsampled for illustration purposes only. We are referencing our readers to established guidelines of sampling rate and recording length [@quintana2016guidelines].

The data contains 2.5 minutes of signals recorded at a frequency of 100Hz. It contains four timeseries representing our three physiological modalities and a photosensor (which signal decreased when a stimulus was displayed on the screen).

<!--I think we should label sections like : Step 1., Step 2, etc. then refer to it in the text
Step 1. -->
```{python eventrelated_data, include=TRUE, eval=TRUE, echo = TRUE}
# Load the package
import neurokit2 as nk

# Download example dataset
data = nk.data("bio_eventrelated_100hz")

# Visualize 10 seconds of data (on the same scale)
nk.signal_plot(data[900:1900], standardize=True)
```
<!--here I think the figure is great, but it seems like an arbitrary slice is unimportant compared to epoch_plot()-->
```{r include=TRUE, eval=TRUE, echo = FALSE, fig.width=10, fig.height=6, fig.cap="Subset of the dataset showing one event (in orange) and the other physiological signals."}
py$data %>%
  standardize() %>%
  mutate(Time = 1:n() / 100) %>%
  slice(900:1900) %>%
  pivot_longer(1:4) %>%
  mutate(name = fct_relevel(name, c("ECG", "RSP", "EDA", "Photosensor"))) %>%
  ggplot(aes(x=Time, y=value, color=name, size=name)) +
  geom_line() +
  theme_modern() +
  scale_color_manual(values=c("ECG"="red", "EDA"="#9C27B0", "RSP"="#2196F3", "Photosensor"="#FF9800")) +
  scale_size_manual(values=c("ECG"=0.66, "EDA"=2, "RSP"=2, "Photosensor"=2)) +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        legend.title = element_blank(),
        legend.position = "top") +
  ylab("Time (s)") +
  guides(size = guide_legend(override.aes = list(size = 2)))
```
<!--Step 2.-->
```{python eventrelated_analysis, include=TRUE, results="hide", eval=TRUE, echo = TRUE}
# Process the data
df, info = nk.bio_process(ecg=data["ECG"],
                          rsp=data["RSP"],
                          eda=data["EDA"],
                          sampling_rate=100)

# Find events
conditions = ["Negative", "Neutral", "Neutral", "Negative"]
events = nk.events_find(event_channel=data["Photosensor"],
                        threshold_keep='below',
                        event_conditions=conditions)

# Epoch the data
epochs = nk.epochs_create(data=df,
                          events=events,
                          sampling_rate=100,
                          epochs_start=-0.1,
                          epochs_end=4)
```
<!-- why don't we plot epochs here instead of plotting an arbitrary slice ?
Step 3.-->
```
# Extract event related features
results = nk.bio_analyze(epochs)

# Show subset of results
results[["Condition", "ECG_Rate_Mean", "RSP_Rate_Mean", "EDA_Peak_Amplitude"]]
```

```{r table1word, eval=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# For word
knitr::kable(py$results[c("Condition", "ECG_Rate_Mean", "RSP_Rate_Mean", "EDA_Peak_Amplitude")], format="markdown", digits = 3, caption = "Subset of the ouput related to event-related analysis characterizing the pattern of physiological changes related to specific stimuli.", row.names = FALSE)
```
```{r table1pdf, eval=TRUE, message=FALSE, warning=FALSE, echo=FALSE, out.width = "\\textwidth", fig.pos = "!ht", results = "asis"}
# For PDFs
kable(py$results[c("Condition", "ECG_Rate_Mean", "RSP_Rate_Mean", "EDA_Peak_Amplitude")], format="latex", digits = 2, booktabs = TRUE, caption = "Subset of the ouput related to event-related analysis characterizing the pattern of physiological changes related to specific stimuli.", linesep="") %>%
  kableExtra::kable_styling(latex_options = c("hold_position"))
```


Event-related paradigms are exploited oftenly. Though, standardized and reproducible pipelines for these analyses are still not common practice for biosignals. In step 1, our example dataset is loaded along with the package. In step 2, each channel contained the dataset are processed. While physiological modalities have their dedicated *high-level wrapper function*, data from the photosensor is processed separetely with `events_find()`. Conditions can be easily intialized by giving a list representing the order of presentation of stimuli to `event_conditions` argument. The timeseries can then be segmented into epochs corresponding to a specified window around each event. That way, `epochs_create()` will take the processed data in `df` and keep a dictionary containg only segments of the dataframe corresponding to the time window around `events`. In step 3, relevant features were extracted by giving our `epochs` dictionary of dataframes to `bio_analyze()`. These features include the changes in rates of ECG and RSP signals (e.g. maximum, minimum and mean rate after stimulus onset, the time at which they occur), the peak characteristics of EDA signal (e.g., occurrence of skin conductance response (SCR), and if SCR is present, its corresponding peak amplitude, time of peak, rise and recovery time). In addition, for ECG and RSP signals, the information of the respiration and cardiac phases are also extracted (i.e.,  the respiration phase - inspiration/expiration - and cardiac phase - systole/diastole - occuring at the onset of event). This example shows how simple it is to extract features of physiological responses. In fact, it should be clear that this pipeline can also scale up with little or no effort to group-level analysis by aggregating the mean of features accross subjects.  Using NeuroKit to analyze public databases of event-related experiments is a logical next step.

<!--Description is clear, now we need to say how this pipeline facilitates valid interpretations and how it reveals crucial physiological processes that are linked to cognition and affectivity-->


## Resting-state Features

This dataset represents a human at rest (eyes-closed in a sitted position). It contains three channels (ECG, PPG and RSP) recorded at a frequency of 100Hz during five minutes. In this example, one would be interested in extracting features of rate variability and longer-term activations.

```{python intervalrelated, include=TRUE, results="hide", eval=TRUE, echo = TRUE}
# Load the package
import neurokit2 as nk

# Download example dataset
data = nk.data("bio_resting_5min_100hz")

# Process the data
df, info = nk.bio_process(ecg=data["ECG"],
                          rsp=data["RSP"],
                          sampling_rate=100)

# Extract features
results = nk.bio_analyze(df)

# Show subset of results
results[["ECG_Rate_Mean", "ECG_HRV_RMSSD", "RSP_Rate_Mean", "RSA_P2T_Mean"]]
```

```{r table2word, eval=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# For word
knitr::kable(py$results[c("ECG_Rate_Mean", "ECG_HRV_RMSSD", "RSP_Rate_Mean", "RSA_P2T_Mean")], format="markdown", digits = 3, caption = "Subset of properties characterizing the physiological activity over a period of 5 minutes of resting-state", row.names = FALSE)
```
```{r table2pdf, eval=TRUE, message=FALSE, warning=FALSE, echo=FALSE, out.width = "\\textwidth", fig.pos = "!ht", results = "asis"}
# For PDFs
kable(py$results[c("ECG_Rate_Mean", "ECG_HRV_RMSSD", "RSP_Rate_Mean", "RSA_P2T_Mean")], format="latex", digits = 2, booktabs = TRUE, caption = "Subset of properties characterizing the physiological activity over a period of 5 minutes of resting-state.", linesep="") %>%
  kableExtra::kable_styling(latex_options = c("hold_position"))
```

The dataset has been processed, and then passed to the analysis function, which extracted properties of physiological activity, such as the rate characteristics of ECG and RSP signals (e.g. the mean and the variability of the heart and breathing rate). <!--Extend and clarify description as with the other one by sectionning-->

<!--Description is clear, now we need to say how this pipeline facilitates valid interpretations and how it reveals crucial physiological processes that are linked to cognition and affectivity-->


# Conclusion and Future Directions

Despite not having a Graphical User Interface (GUI), *NeuroKit2* is accessible to people with very little knowledge of python or programming in general, thanks to its design choices focusing on user-experience.

Future evolution will mostly be driven by the community and the advances in the field. Possible directions include extending the support for other types of bodily signals (e.g., electrogastrography - EGG, electrooculography - EOG) and strenghtening the efficiency of the code to obtain performance gains for large datasets.

# Conflict of Interest

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

# Acknowledgements

We would like to thank all the contributors (https://neurokit2.readthedocs.io/en/latest/authors.html), and the users for their support.








\newpage

# References
```{r create_r-references}
r_refs(file = "bibliography.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
