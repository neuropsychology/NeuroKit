---
title: '**NeuroKit2: A Python Toolbox for Neurophysiological Signal Processing**'
shorttitle        : "NeuroKit2"
author:
  - name          : "Dominique Makowski"
    affiliation   : " 1,*"
    corresponding : no    # Define only one corresponding author
    address       : "HSS 04-18, 48 Nanyang Avenue, Singapore"
    email         : "dmakowski@ntu.edu.sg"
  - name          : "Tam Pham"
    affiliation   : " 1"
  - name          : "Zen J. Lau"
    affiliation   : " 1"
  - name          : "Jan C. Brammer"
    affiliation   : " 2"
  - name          : "Hung Pham"
    affiliation   : " 3"
  - name          : "Francois Lespinasse"
    affiliation   : " 4"
  - name          : "Christopher Sch\\\"{o}lzel"
    affiliation   : " 5"
  - name          : "S.H. Annabel Chen"
    affiliation   : " 1, 6, 7"
affiliation:
  - id            : "1"
    institution   : "School of Social Sciences, Nanyang Technological University, Singapore"
  - id            : "2"
    institution   : "???"
  - id            : "3"
    institution   : "???"
  - id            : "4"
    institution   : "Departement de psychologie, Universite de Montreal, Montreal, Canada"
  - id            : "5"
    institution   : "Life Science Informatics, THM University of Applied Sciences, Gisslen, Germany"
  - id            : "6"
    institution   : "Centre for Research and Development in Learning, Nanyang Technological University, Singapore"
  - id            : "7"
    institution   : "Lee Kong Chian School of Medicine, Nanyang Technological University, Singapore"
authornote: |
  * Correspondence concerning this article should be addressed to Dominique Makowski (HSS 04-18, 48 Nanyang Avenue, Singapore; dmakowski@ntu.edu.sg).
abstract: |
   NeuroKit2 is an open-source, user-friendly Python package dedicated to neurophysiological signal processing. It developed from a collaborative project aimed at offering programming ease both for novice and advanced users to perform elaborate analyses of electrocardiogram (ECG), respiratory (RSP), electrodermal activity (EDA), and electromyography (EMG) data. It comprises of a consistent set of user-friendly, high-level functions that implement an all-in-one cleaning, preprocessing, and processing pipeline with sensible defaults. At the same time, greater flexibility and parametric control can be achieved by using Neurokit2's mid-level functions to build a custom analysis pipeline. With the current lack of open-source tools needed for comprehensive signal processing, Neurokit2 presents new opportunities for researchers to move away from unreadable black-box algorithms and towards greater control over their results, improving reproducibility in research. (talk about novelty? or cutting-edge measure extraction? or reproducibility?)
keywords          : "Neurophysiology, Biosignals, Python, ECG, EDA, EMG, RSP"
wordcount         : ""
bibliography      : ["bibliography.bib"]
floatsintext      : yes
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output:
  papaja::apa6_pdf:
    keep_tex: FALSE
    latex_engine: xelatex
  papaja::apa6_word:
    keep_tex: FALSE
header-includes:
   - \usepackage[labelfont=bf, font={color=gray,small}]{caption}
   - \usepackage{float}
   - \usepackage[document]{ragged2e}
editor_options:
  chunk_output_type: console
---

\justify

```{r r_setup, include = FALSE, warning=FALSE, message=FALSE}
library("papaja")
library("kableExtra")
options(knitr.kable.NA = 'None')

library(tidyverse)
library(easystats)

# Setup python - you need to change the path to your python distribution
library(reticulate)
reticulate::use_python("D:/Downloads/WPy64-3810/python-3.8.1.amd64/")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
```



<!-- Research gap -->
Cognitive neuroscience and psychology is increasingly relying on neurophysiological methods to assess brain and bodily activity. Aside from theorethical considerations motivating this shift (e.g., the growth of embodied or affective neuroscience, @Kiverstein2015),<!--I think here it's best to keep it simple and general-->, practical reasons can include low monetary cost (especially compared with other imaging techniques, such as MRI or MEG), high user convenience (e.g., portability, setup speed) and wide availability (e.g., in "smart" health devices; @yuehong2016internet). At the same time, the fields of signal processing and computational data science continue to grow, pushing the horizon of possibilities and resulting in the emergence of a mryiad of new processing algorithms [@clifton2012machine]. <!--Most importantly, open-sourced benchmarking... this sentence was bit vague -->However, as these methods are often not easily accessible and user-friendly, neurophysiological data processing remains a challenge for many researchers without a formal training or experience in programming.

<!-- Response to gap -->
*NeuroKit2* aims at addressing this challenge by offering a free, user-friendly, and complete solution for neurophysiological data processing. It is an open-source Python package, developed in a collaborative environment that continues to welcome contributors from different countries and fields. Historically, *NeuroKit2* is the re-forged successor *NeuroKit.py* (*https://github.com/neuropsychology/NeuroKit.py*), a PhD side project that ended up attracting a lot of users (248 GitHub stars as of 09-04-2020). The new version takes on its best features and design choices, and re-implements them in a professional and well-thought way. It aims at being 1) accessible and well-documented, 2) reliable and cutting-edge, and most importantly, 3) flexible and powerful.<!--robust can be interpreted in different ways-->

<!-- Accessibility and documentation -->
The package is available for Python 3 [@python3] and thus users benefit from <!--the package itself doesn't benefit from users--> existing tutorials and a large online community. It is also relatively lightweight, using mainly standard dependencies [@scipy] such as *NumPy*, *pandas*, *SciPy*, *scikit-learn* and *MatplotLib* (with an additional system of optional dependencies), enabling its use as a dependency in other software. The package source code is available under a permissive license on GitHub (*https://github.com/neuropsychology/NeuroKit*); along with its documentation, automatically built and hosted at *https://neurokit2.readthedocs.io/*. Documentation follows a standardized structure [@sphinx-tobe-added] in order to be rendered directly from the code. Apart from a guide for installation as well as contribution, and a decription of the package's functions, the documentation also includes several cloud-based interactive examples. These are built in a tutorial-like manner, providing a walk-through on how to address specific issues (for instance, how to extract and visualize individual heartbeats). New examples can be easily added by users simply by uploading a Python notebook file (`.ipynb`) to the repository. This notebook will then be automatically rendered in a static form in the documentation webpage, and also accessible to everyone who wishes to interact with its content (no prior installation needed) via a *Binder* environment [@Jupyter2018]. <!--(maybe referencing our binder would be a good idea at this point) This notebook file will be automatically transformed into a webpage and displayed on the website--> This ensures a transparent and evolutive documentation. The accessibility for newcomers is also reinforced by the issue tracker of GitHub, where users can create public issues to inquire for help.

<!-- Reliability and Evolution -->
The package aims at being reliable and trustworthy, that is by continously implementing new peer-reviewed processing and analyses pipelines. It also means that the contributors are dedicated to ensuring its stability using continuous integration tools like Travis [@travis-tobe-added]. *NeuroKit2's* functions are also tested against existing implementations of established reference software such as *BioSPPy* [@biosppy], *hrv* [*under review*](https://github.com/openjournals/joss-reviews/issues/1867), *PySiology* [@PySiology], *HeartPy* [@HeartPy], *systole* [@Systole] or *nolds* [@nolds]. The code itself includes a comprehensive test suite to ensure stability and prevent error. Moreover, the issue tracker allows users to easily report any bugs and track their fixation. Thanks to its collaborative and open development, as well as its modular organization, *NeuroKit2* is currently being developed with a long-term perspective in mind, with hopes of remaining cutting-edge through its ability to evolve, adapt, and integrate new methods as they emerge.

<!-- - Powerful and flexible: API -->
Finally, we believe that the design philosophy contributes to a robust  <!--powerful or robust | parentheses at end of sections are not super great; pushing them in the next ?--> yet flexible user interface (API), which is described below. In order to demonstrate this design, the most common use-cases of NeuroKit are presented through examples using Python scripts, namely with data from event-related and resting state paradigms. We will conclude by discussing how those examples reveal the way *Neurokit2* addresses reproducibility issues in neurophysiological research. We will then open up on a more foundational and theoretical note by appreciating how its accessibility and continuous integration could benefit to new-coming applications of these methods. <!--foundational and theoretic issues : to answer to first statement which is about the growing use of this methods to describe human behavioral, cognitive and affective processes [embodiement, intricacies affectivity and cognition]-->

# Design Philosophy

We cannot stress this enough: *NeuroKit2* aims at being accessible to beginners and, at the same time, offering a maximal level of control to experienced users. Robustness and flexibility of the API are achieved by allowing beginning users to implement complex processing and analyses pipelines with very few functions, while still enabling fine-tuned control and precision to more experienced users. This trade-off is allowed by the implementation of 3 abstract levels of functions.


## Low-level: Signal Processing Base Utilities

The basic building blocks are functions to facilitate general signal processing, i.e., to do filtering, resampling, interpolating, peak detection, etc. These functions are signal-agnostic, and include a lot of tweakable parameters. For instance, one can change the filtering method, frequencies, order etc. Most of these functions are based on validated algorithms present in *scipy* [@scipy]. Examples of such functions include `signal_filter()`, `signal_interpolate()`, `signal_resample()`, `signal_detrend()`, and `signal_findpeaks()`.

## Mid-level: Neurophysiological Processing Steps

The signal processing utilities are then used by functions specific to the different types of physiological signals. These functions aim at taking care of specific steps of physiological data processing, such as cleaning, peak detection, phase classification or rate computation. Critically, for each type of signals (ECG, RSP, EDA, EMG, and PPG), the same function names are called (in the form `signaltype_functiongoal()`) to achieve equivalent goals, such as `*_clean()`, `*_findpeaks()`, `*_process()`, `*_plot()` (replace the star with the signal type, e.g., `ecg_clean()`), making it intuitive and consistent accross different modalities.

For example, the `rsp_clean()` function uses `signal_filter()` and `signal_detrend()`, with different possible sets of default parameters that can be switched via a "method" argument (corresponding to different published or validated pipelines). For instance, setting `method="khodadad2018"` will use the cleaning workflow described in @khodadad2018optimized. If a user wants to build its own custom cleaning function, he or she can use a `*_clean()` function as a template to choose the appropriate set of parameters in low-level signal processing tools (this process is also examplified in the documentation).


## High-level Wrappers for Processing and Analysis

Lastly, these steps are assembled in front-end <!--what does this mean?--> "master" functions. For instance, the `ecg_process()` function uses `ecg_clean()`, `ecg_findpeaks()`, `ecg_rate()`. A specific processing pipeline can be selected via the `method` argument of the given function. This `method` is propagated throughout its different subsets in lower-level functions. These modality-specific processing functions allow begginners to compare the outcomes of different pipelines on their data. It facilitates and streamlines important but time-consuming steps in reproducible research such as the validation of data preparation and quality control [@Quintana2016].

<!--Last but not least ; not sure this expression is quite the tone we're looking for--> Finally, the package includes meta-functions (e.g., `bio_process`) that allows the combined processing of multiple types of signals at once. As a result, <!--the existence of--> these high-level functions allows for reproducible and easy-to-execute biosignal processing pipelines. As NeuroKit performs preprocessing and processing with sensible defaults, researchers who are not comfortable in programming or who have not yet identified a specific pipeline they want, can run the default pipeline with one line of code (e.g., `bio_process(ecg=ecg_signal, eda=eda_signal, method='neurokit')`). We find that this can be rewarding for begginners who despite prior expertise in programming are able to perform cutting-edge feature extractions and visualizations. In fact, for these beginners, it may demystify the usage of "pure" programming tools for their analyses (as opposed to statistics and biosignal software such as SPSS, Kubios or Acqknowledge). In contrast to software-specific pipelines, this may make *NeuroKit2* attractive for beginners as it provides an all-in-one solution while also providing opportunities for them to understand the frontend, backend and the in-betweens of signal processing,  Moreover, this solution facilitates replication of research steps.

Importantly, more advanced users can very easily build their own custom analysis pipeline by using the mid-level functions. Thus, the code structure offers more control over the parameters of pipelines at the cost of learning Python. We believe this calibrated trade-off between flexibility and robustness can encourage some researchers to become a part of a supportive community construed of many expertises rather than rely on proprietary softwares <!--as their go-to pipelines--> to achieve their goals.


# Example


We will present two examples that illustrate the most common use cases of *Neurokit2*. Their implementations are outlined below with their corresponding datasets. Firstly, we will show how simple it is to compute contrasts between experimental conditions using data from an event-related paradigm. In this example, one would be interested in the momentarily short-term physiological responses to some specific stimuli. The second example provided shows how *Neurokit2* can extract longer-term characteristics (features) of physiological activity, namely Heart Rate Variability (HRV). Datasets are provided along with the package, and examples are available in their interactive and static versions.


## Event-related Paradigm

The `event related dataset` contains ECG, RSP and EDA signals taken from one participant to whom was presented four emotional images (from the NAPS database; @marchewka2014nencki). The dataset was shortened and downsampled for illustration purposes only. We are referencing our readers to established guidelines of sampling rate and recording length [@Quintana2016]. The data contains 2.5 minutes of signals recorded at a frequency of 100Hz. It contains four timeseries representing our three physiological modalities and a photosensor (of which the signal decreased whenever a stimulus was displayed on the screen).

<!--I think we should label sections like : Step 1., Step 2, etc. then refer to it in the text
Step 1. -->
```{python eventrelated_data, include=TRUE, eval=TRUE, echo = TRUE}
# Load the package
import neurokit2 as nk

# Download example dataset
data = nk.data("bio_eventrelated_100hz")

# Visualize 10 seconds of data (on the same scale)
nk.signal_plot(data[900:1900], standardize=True)
```
<!--here I think the figure is great, but it seems like an arbitrary slice is unimportant compared to epoch_plot()-->
```{r include=TRUE, eval=TRUE, echo = FALSE, fig.width=10, fig.height=6, fig.cap="Subset of the dataset showing one event (in orange) and the other physiological signals."}
py$data %>%
  standardize() %>%
  mutate(Time = 1:n() / 100) %>%
  slice(900:1900) %>%
  pivot_longer(1:4) %>%
  mutate(name = fct_relevel(name, c("ECG", "RSP", "EDA", "Photosensor"))) %>%
  ggplot(aes(x=Time, y=value, color=name, size=name)) +
  geom_line() +
  theme_modern() +
  scale_color_manual(values=c("ECG"="red", "EDA"="#9C27B0", "RSP"="#2196F3", "Photosensor"="#FF9800")) +
  scale_size_manual(values=c("ECG"=0.66, "EDA"=2, "RSP"=2, "Photosensor"=2)) +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        legend.title = element_blank(),
        legend.position = "top") +
  ylab("Time (s)") +
  guides(size = guide_legend(override.aes = list(size = 2)))
```
<!--Step 2.-->
```{python eventrelated_analysis, include=TRUE, results="hide", eval=TRUE, echo = TRUE}
# Process the data
df, info = nk.bio_process(ecg=data["ECG"],
                          rsp=data["RSP"],
                          eda=data["EDA"],
                          sampling_rate=100)

# Find events
conditions = ["Negative", "Neutral", "Neutral", "Negative"]
events = nk.events_find(event_channel=data["Photosensor"],
                        threshold_keep='below',
                        event_conditions=conditions)

# Epoch the data
epochs = nk.epochs_create(data=df,
                          events=events,
                          sampling_rate=100,
                          epochs_start=-0.1,
                          epochs_end=4)
```
<!-- why don't we plot epochs here instead of plotting an arbitrary slice ?
Step 3.-->
```
# Extract event related features
results = nk.bio_analyze(epochs)

# Show subset of results
results[["Condition", "ECG_Rate_Mean", "RSP_Rate_Mean", "EDA_Peak_Amplitude"]]
```

```{r table1word, eval=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# For word
knitr::kable(py$results[c("Condition", "ECG_Rate_Mean", "RSP_Rate_Mean", "EDA_Peak_Amplitude")], format="markdown", digits = 3, caption = "Subset of the ouput related to event-related analysis characterizing the pattern of physiological changes related to specific stimuli.", row.names = FALSE)
```
```{r table1pdf, eval=TRUE, message=FALSE, warning=FALSE, echo=FALSE, out.width = "\\textwidth", fig.pos = "!ht", results = "asis"}
# For PDFs
kable(py$results[c("Condition", "ECG_Rate_Mean", "RSP_Rate_Mean", "EDA_Peak_Amplitude")], format="latex", digits = 2, booktabs = TRUE, caption = "Subset of the ouput related to event-related analysis characterizing the pattern of physiological changes related to specific stimuli.", linesep="") %>%
  kableExtra::kable_styling(latex_options = c("hold_position"))
```

<!--Though event-related paradigms are often employed, standardized and reproducible pipelines for biosignal analyses are still not common practice. As a symbol of this lack of consensus, the chapter dedicated to biosignal processing in the go-to *Handbook of psychophysiology* does not even mention this issue [@Gratton2017] : Tone feels abit harsh, maybe rephrase and say why it is important. Also perhaps this paragraph if needed should be after the example is explained->. <!--We are suggesting--> Here, event-related features can be extracted with three simple steps<!--to illustrate our approach in solving this problem-->. In step 1, our example dataset is loaded along with the package. In step 2, each channel contained in the dataset is processed. While physiological modalities have their dedicated *high-level wrapper function*, data from the photosensor is processed separetely with `events_find()`. Conditions can be easily intialized by giving a list representing the order of presentation of stimuli to the `event_conditions` argument. Timeseries can then be segmented from the data into epochs corresponding to the specified window around each event. That way, `epochs_create()` will take the processed data in `df` and keep a dictionary containing only segments of the dataframe corresponding to the time window around `events`. In step 3, relevant features are computed by giving our `epochs` dictionary of dataframes to `bio_analyze()`. The features include the changes in rates of ECG and RSP signals (e.g. maximum, minimum and mean rate after stimulus onset, and the time at which they occur), and the peak characteristics of EDA signal (e.g., occurrence of skin conductance response (SCR), and if SCR is present, its corresponding peak amplitude, time of peak, rise and recovery time). In addition, respiration and cardiac cycle phases are also extracted (i.e.,  the respiration phase - inspiration/expiration - and cardiac phase - systole/diastole - occuring at the onset of event).

This example shows the straightforward process of extracting short-term features of physiological responses/activity. <!--In fact, it should be clear that--> This pipeline can also scale up with little-or-no effort to group-level analysis by aggregating the mean of features accross subjects. We believe these steps are easy for beginners to memorize and implement, <!-- as well as intuitive--> and are sufficiently flexible for the more advanced to tweak parameters to suit their desired goal. <!-- One might say that it could speed up the process between data collection and analysis-->. 

In addition to streamlining data analyses, this pipeline also translates into richer and more meaningful <!--more valid--> interpretations of physiological activity and how differential physiological activations are linked to the events of interest, all with high temporal precision. In the example here, exposure to negative stimuli results in stronger cardiac deceleration, higher skin conductance response, and accelerated breathing rate, as compared to neutral stimuli. *Neurokit2's* signal processing pipeline is thus able to delineate such physiological processes that are crucial in generating conclusions about spontaneous or stimulated cognition and affectivity alongside behavioural data. We believe that using *NeuroKit2* to analyze public databases of event-related experiments is a logical next step to ensure that its usability is replicable.

<!--Description is clear, now we need to say how this pipeline facilitates valid interpretations and how it reveals crucial physiological processes that are linked to cognition and affectivity-->


## Resting-state Features

The `interval related dataset` represents physiological activity of a human at rest (eyes-closed in a sitted position). It contains three channels (ECG, PPG and RSP) sampled at a frequency of 100Hz during five minutes. In this example, one would be interested in computing features of rate variability and longer-term activation patterns.

<!--Step 1. -->
```{python intervalrelated, include=TRUE, results="hide", eval=TRUE, echo = TRUE}
# Load the package
import neurokit2 as nk

# Download example dataset
data = nk.data("bio_resting_5min_100hz")

# Process the data
df, info = nk.bio_process(ecg=data["ECG"],
                          rsp=data["RSP"],
                          sampling_rate=100)
```
<!--Step 2 NOTE: I would add ecg_hrv(), because if I remember correctly, HRV is not computed by bio_process() -->
```
# Compute Rate Variability
hrv = nk.ecg_hrv(ecg_rate=df["ECG_Rate"], sampling_rate=100, show=True)

# Extract features
results = nk.bio_analyze(df)

# Append results and HRV
results = results.append(hrv)

# Show subset of results
results[["ECG_Rate_Mean", "ECG_HRV_RMSSD", "RSP_Rate_Mean", "RSA_P2T_Mean"]]
```

```{r table2word, eval=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# For word
knitr::kable(py$results[c("ECG_Rate_Mean", "ECG_HRV_RMSSD", "RSP_Rate_Mean", "RSA_P2T_Mean")], format="markdown", digits = 3, caption = "Subset of properties characterizing the physiological activity over a period of 5 minutes of resting-state", row.names = FALSE)
```
```{r table2pdf, eval=TRUE, message=FALSE, warning=FALSE, echo=FALSE, out.width = "\\textwidth", fig.pos = "!ht", results = "asis"}
# For PDFs
kable(py$results[c("ECG_Rate_Mean", "ECG_HRV_RMSSD", "RSP_Rate_Mean", "RSA_P2T_Mean")], format="latex", digits = 2, booktabs = TRUE, caption = "Subset of properties characterizing the physiological activity over a period of 5 minutes of resting-state.", linesep="") %>%
  kableExtra::kable_styling(latex_options = c("hold_position"))
```

In step 1, the dataset is loaded along with the package then processed with the same function previously used. The processing function outputs dataframe `df` including continuous respiratory and heart rate signals in addition to raw and cleaned signals. It also outputs `info` which represents a dictionary of detected peaks and troughs of RSP and ECG signals. In Step 2, HRV is computed over the 5 minutes of processed heart rate signal. It is computed separetely <!--why is it computed separetely?, state reason-->. In parallel, `df` is directly passed to the analysis function, which computes the mean of heart and breathing rate. Results from `ecg_hrv()` and `bio_analyze()` can be appended to the same dataframe, and a subset of them can be chosen for outputting a table.

As resting-state paradigms are widely used in human neuroscience, our analysis pipeline would be a convenient and effective tool to extract relevant features of physiological activity for these experiments. As research shows relations between high-frequency HRV and brain structure and connectivity [@Yoo2018, @Jennings2016], it is only due-process that tested and benchmarked reproducible pipelines become common practice for these analyses. <!--this sentence doesn't seem coherent/understandable enough: why is it relevant that there are relations between high frequency HRV and brain structure and connectivity?--> Otherwise, the accesibility of recording instruments combined together with this easy to run pipeline can <!--obviously--> be interesting to amateurs who wish to learn to assess their own health status. Perhaps, professionnals who use biofeedback systems could benefit from a stable pipeline for their data processing. And even further, one could envision psychotherapy and meditation sessions assisted by these sytems. <!-- paragraph's tone also needs revision-->

# Conclusion and Future Directions

Despite not having a Graphical User Interface (GUI), *NeuroKit2* is accessible to people with very little knowledge of python or programming in general, thanks to its design choices focusing on user-experience. Naturally, our design philosophy extends to our collaborative community in developing this package. *Neurokit2* is a pragmatic solution to a problem beyond specific research questions: it addresses the need for transparent and accessible methods in neurophysiology. We believe this will not only help researchers answer important scientific questions, but it will also allow for less experienced users to build experimentations and applications that they sought to conceptualize, but had difficulty materializing it <!--just never thought they could-->.

Future evolution will mostly be driven by the community and the advances in related fields. Possible directions include extending the support for other types of bodily signals (e.g., electrogastrography - EGG, electrooculography - EOG)<!--I'd probably add something about phys2bids, once this package is released I would write a function to import bidsified bio-data--> and strenghtening the efficiency of the code to obtain performance gains for large datasets. As previously mentioned, testing *Neurokit2* against existing pipelines using public databases from diverse fields would be a logical next step.

Also, we can only hope that practitioners will feel confortable in integrating those methods to more unorthodox areas of applications. This idea did not originate yesterday. More than four decades ago, @Lazarus1975 mentioned the importance of biofeedback in clinical contexts to support self-regulation, i.e. because it offers "informational aid" about bodily processes and helps in recognizing that they can be "volitionally regulated". We believe this approach can speak to many practitioners who are interested by these tools, but did not get proper training in programming.

Thanks to its well-documented API and reliable community, begginners can implement our tools in an informed manner, while being supported by people of diverse backgrounds. Taken together, we have to acknowledge the importance of investigating bodily processes to provide novel insights to human psychological research. In fact, biosignals and related complexity measurements have significant potential to inform models of brain activity and is essential in <!--decribing activity from--> understanding the autonomic nervous system [@Valenza2020]. Perhaps the full integration of these methods will help reveal inner-workings of the intricacies between cognitive, affective and behavioral processes.

All in all, we <!--think--> believe that *Neurokit2* provides essential tools for novice and senior researchers, amateurs, and tech-enthusiasts to investigate human health as well as (neuro)psychological processes at different scales. From "smart health devices" to academic research-grade equipment, *Neurokit2* relies on reproducible data processing and analysis, in addition to a community dedicated to educating one another on related topics.

# Conflict of Interest

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

# Acknowledgements

We would like to thank all the contributors (https://neurokit2.readthedocs.io/en/latest/authors.html), and the users for their support.








\newpage

# References
```{r create_r-references}
r_refs(file = "bibliography.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
